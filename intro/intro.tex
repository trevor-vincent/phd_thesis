%REFERENCES used
%https://arxiv.org/pdf/1803.07965.pdf
%https://arxiv.org/pdf/1710.05832.pdf
%https://journals.aps.org/prd/pdf/10.1103/PhysRevD.95.024029 
%Appendix D in Rezolla hydrodynamics good for Tabulated Equation of State and geometrized units
%Constraints on the neutron star equation of state from AT2017gfo using radiative transfer simulations -> good for intro paragraph
% Evaluating radiation transport errors in merger simulations using a Monte-Carlo algorithm
% https://arxiv.org/abs/1805.11581 GW170817: Measurements of neutron star radii and equation of state
% wyatts paper
% https://www.fis.unipr.it/gravity/HomePages/Thesis/PhDthesis_2009_SebastianoBernuzzi.pdf
% https://arxiv.org/pdf/1804.06308.pdf

\chapter{Introduction}
\label{chap:intro}

The initial proposals for gravitational wave interferometers were
put together in the 1980s. The scientific justification for the interferometers was based on detecting the inspiral and merger of compact-object binaries. Since then, the LIGO (\cite{ligo2018gwtc}), Virgo (\cite{acernese2015advanced}), and GEO600 (\cite{affeldt2014advanced}) detectors have been developed and operated as a network from 2005 to 2010. No detections were made during this initial stage of sensitivity. Alongside the early-2000 interferometer development, numerical simulations of the Einstein equations were beginning to gather ground. In 2000, the first binary neutron star merger was simulated (\cite{shibata2000simulation}), five years later the first binary black hole merger was simulated (\cite{pretorius2005a}) and finally in 2006 the first binary neutron star - black hole simulations were performed (\cite{shibata2011coalescence}). A variety of numerical relativity groups started forming at this time all around the globe, the Caltech-Cornell-CITA group (SXS; black-holes.org),
the Kyoto group (\cite{nagakura:2014hza}), the RIT group (CCRG; ccrg.rit.edu), to name but a few. These groups started building banks of simulated waveforms to aid in the parameter estimation studies that were expected to follow from the first detections. On September 9th, 2015, in a very unexpected event during a testing run, LIGO detected the gravitational wave from two coalescing 30-$M_\odot$ black holes (\cite{theligoscientific:2016wfe}). Since this day, there have been multiple binary black hole waveform detections (\cite{ligo2018gwtc}). More recently however, LIGO made a landmark detection, GW170817 (\cite{abbott2017gw170817}). GW170817
coincided with the detection of a gamma ray burst, GRB 170817A and a series of observations that followed across the electromagnetic spectrum (\cite{villar:2017wcc}). The inferred masses of the bodies and the variety of electromagnetic observations imply that the source was a neutron star binary.

With these detections, and the awarding of the Nobel prize to three LIGO members, the gravitational era of astronomy has been born. Much more is expected in the future, with the upcoming earth-based detectors KAGRA (\cite{somiya2012detector}) and LIGO-India (gw-indigo.org), there will be a worldwide detector network capable of precise source localization. Furthermore, there are already plans for third generation earth-based detectors such as the Einstein Telescope (\cite{einsteintelescope}) and space based detectors such as LISA (elisascience.org). With more and more detectors, both space-based and earth-based, we will be able to gather data from the full frequency spectrum of gravitational waves coming towards us.

Even though numerical relativity has matured greatly since the major developments of the early 2000's, there is still a lot more that needs to be done. The biggest problems today in numerical relativity are the simulation of compact object binaries with matter and the simulation of supernovae, both requiring a great amount of microphysics, multi-scale grids, large sets of complex non-linear PDES, fast supercomputers and modern numerical techniques. With the coming of the exascale age of supercomputing there is an increasingly realistic chance of simulating these systems with all the known microphysics. This thesis aims to make progress in two distinct areas of numerical relativity. First, we seek to improve the computational techniques used to solve the Einstein field equations so that more realistic microphysics may be introduced into the simulations as computing power reaches exascale and beyond. Secondly, we seek to further probe the parameter space of binary neutron star simulations with one the most state of the art numerical relativity codes to help understand the emission properties of these LIGO sources for the next generation of detections. Chapter 2 of this thesis will address the former problem, Chapter 3 will address the latter. Finally, the remaining portion of this introductory chapter will briefly describe the physics and computational techniques needed to understand the main text. 

\section{Solving the Einstein Field Equations on a Supercomputer}

In their usual form, space and time are treated on an equal footing in the Einstein equations. From the perspective of performing a numerical evolution however, we need to reformulate the problem as initial value problem where we have a set of initial gravitational and matter data at some time $t$, and a set of evolution equations which we can use to get the updated data at some other time. To do this, we make the ansatz that space-time can be treated as a time sequence of spatial hypersurfaces. With this ansatz, the space-time metric $g_{\mu\nu}$ can be decomposed into

\begin{equation}
\label{eq:4}
ds^{2} = g_{\mu\nu}dx^{\mu}dx^{\nu} = -\alpha^2dt^2 + \gamma_{ij}(dx^i+\beta^idt)(dx^j+\beta^jdt),
\end{equation}

where the spatial metric $\gamma_{ij}$ is a function of the spatial coordinates $x^{i}$ and $t$ and $\alpha$ is the lapse function that measures proper time between neighboring hypersurfaces along their timelike unit normals $n^{\mu}$ and $\beta^i$ is the shift vector that determines how coordinate labels move between each hypersurface. This is known as the 3+1 decomposition of the metric (\cite{arnowitt2008republication}).

Now we need to decompose the Einstein field equations into a set of evolution and constraint equations involving the quantities $\beta^i$, $\alpha$ and $\gamma_{ij}$. To do this, we introduce the projection operator $\perp^{\alpha}_{\beta} = \delta^{\alpha}_{\beta} + n^{\alpha} n_{\beta}$, which can be easily proven to project the space-time components orthogonal to $n^{\mu}$ out of any space-time vector.

The three possible projections of the EFEs: $n^{\mu} n^{\nu} G_{\mu\nu} = 8\pi n^{\mu} n^{\nu} T_{\mu\nu}$, $n^{\mu}\perp^{\nu}_{\delta}G_{\mu\nu} = 8\pi n^{\mu} \perp^{\nu}_{\delta} T_{\mu\nu}$, $\perp^{\mu}_{\rho} \perp^{\nu}_{\delta} G_{\mu\nu} = 8\pi \perp^{\mu}_{\rho} \perp^{\nu}_{\delta} T_{\mu\nu}$ lead to the three York/ADM 3+1 equations respectively:
%
\begin{align}
0 &= R + K^{2} - K^{mn}K_{mn} -16\pi\rho, \\
0 &= D_iK - D_mK^m_i + 8\pi j_i, \\
\partial_t K_{ij} &= \beta^m\partial_mK_{ij} + K_{mj}\partial_i\beta^m + K_{im}\partial_j\beta^m - D_iD_j\alpha \\ 
&+ \alpha(R_{ij}+KK_{ij}-2K_{im}K^m_{j})+4\pi\alpha[(S-\rho)\gamma_{ij}-2S_{ij}],
\end{align} 
%
where $K_{ij} = \beta^m\partial_m\gamma_{ij} + \gamma_{mj}\partial_{i}\beta^{m} + \gamma_{im}\partial_j\beta^{m}-\partial_{t}\gamma_{ij}$, is called the extrinsic curvature and it measures
the rate at which the hypersurface deforms as it is carried forward along a normal (\cite{baumgarte2010numerical}). We also relabelled the projections of the stress tensor by $\rho = T_{\mu\nu} n^{\mu} n^{\nu}$, $j_{\alpha}= -\perp^{\nu}_{\alpha} T_{\mu\nu} n^{\nu}$, $S_{\alpha\beta} = \perp^{\mu}_{\alpha} \perp^{\nu}_{\beta} T_{\mu\nu}$ and $S=\gamma^{\mu\nu} S_{\mu\nu}$, while $R_{ij}$ and $R$ denote the Ricci tensor and scalar associated with $\gamma_{ij}$.

The 3+1 equations are a set of 10 equations, or 3 tensor equations. The 1st equation is known as the Hamiltonian constraint equation. The second tensor equation is called the momentum constraint equation and is composed of three equations. In total, these four elliptic constraint equations play a similar role as the equations $\nabla \cdot \vec E = 4\pi\rho$ and $\nabla \cdot \vec B = 0$ which constrain the initial $\vec E$ and $\vec B$ fields in Maxwell's equations. The constraint equations must be solved prior to evolving the initial data and are usually called the \textbf{initial data equations}. The last set of six equations are the evolution equations. As they stand, the 3+1 equations still need to be manipulated a bit in order to discretize and solve them on a computer. For the set of evolution equations, the most used schemes are BSSN, Z4 and the Generalized Harmonic Decomposition (although the Generalized Harmonic decomposition doesn't start from the ADM equations, see Chapter 2 for more details) which each manipulate the evolution equations into a slightly different well-posed hyperbolic system. For the initial data equations, the most used schemes are conformal tranverse traceless (CTT), conformal-thin sandwich (CTS) and the extended-conformal thin-sandwich (XCTS) frameworks (\cite{pfeiffer-york:2005,alcubierre2012introduction,sopuerta2015gravitational}).

Alongside the field-equations, the matter equations must be solved. These come from the local conservation equations $\nabla_{\mu}T^{\mu\nu} = 0$ (this changes for the case of radiation-hydrodynamics, see Chapter 2). For Neutron-star matter, a perfect-fluid tensor and an irrotational velocity distribution are usually assumed and are coupled with some choice of equation of state (EOS). For black-holes, we set $\rho = j_{\alpha} = S_{\alpha\beta} = 0$ because there is only vacuum space. On top of the matter-equations, the equations of neutrino transport must be solved, we will look at this in detail in Chapter 3.

To extract the gravitational waveform, the Weyl scalar $\psi_{4}$, which represents the outgoing transverse radiation, is extracted at a large radius away from the simulated binary system (the ``wave-zone''). The energy, linear momentum, and angular momentum of the gravitational wave are computed by integrating the $\psi_{4}$ scalar in time (\cite{kyutoku2015dynamical}).

For a more in-depth review, see (\cite{sperhake2014numerical}), (\cite{faber2012binary}) and (\cite{shibata2011coalescence}) for BBH, NSNS and NSBH systems respectively. In the next section we discuss problems that are currently plaguing solvers for the initial data equations and then in the subsequent section we discuss new numerical methods that we plan to use to fix these problems.

\section{Current problems with Einstein Initial Data solvers}

In simulations there are two current approaches to modeling neutron stars with temperature-dependent realistic EOSs in binaries. The first is to use nuclear-theory-based EOSs in tabulated form and to interpolate as required during the simulation. Since this can be computationally expensive, the more common approach is to use $n$ piece-wise polytropes with a polytropic thermal correction (\cite{deaton2013black}), (\cite{kyutoku2013black}), (\cite{bauswein2014revealing}), (\cite{kyutoku2015dynamical}). The cold piece-wise polytropic part is defined as follows,

\begin{equation}
\label{eq:7}
P_{cold} = K_i\rho^{\Gamma_i},\,\,\, \epsilon_{cold} = \epsilon_i + \frac{K_{i}\rho^{\Gamma_{i}-1}}{\Gamma_{i}-1},
\end{equation}

where $P$, $\rho$, $\epsilon$ and $K$ are the pressure, rest-mass density, specific internal energy and the polytropic constant, respectively, with i denoting the $i-th$ polytropic piece. It was found that four polytropic pieces ($n=4$) will approximate most EOSs of cold neutron matter to good accuracy (\cite{read2008neutron}). To include thermal effects when using piecewise polytropes, one splits up the pressure $P = P_{cold} + P_{th}$ and internal energy $\epsilon = \epsilon_{cold}+\epsilon_{th}$ and assumes an ideal gas relationship $P_{th} = \rho \epsilon_{th}(\Gamma_{th}-1)$ where the ideal gas index $\Gamma_{th}$ is constant for all $\rho$ and $\epsilon$ and usually set to 2 (\cite{bauswein2010testing}),(\cite{takami2014constraining}).

 The majority of NS binary simulations to date, which includes the initial data solves, use smooth analytic EOSs such as the unrealistic $\Gamma = 2$ polytrope and stars with low compactness ($C = M_{NS}/R_{NS} \approx .1$) (\cite{faber2012binary}). The main issue with solving the Einstein equations and matter equations with realistic EOSs in tabulated or piecewise-analytic form are that they are not smooth due to the multiple phase transitions and the non-analytic behavior or very steep slopes at the surface of the NS. For example, if we consider the LS220 EOS at low temperature and electron fraction \cite{lattimer1991generalized}, near the NS center the EOS can be approximated by a stiff $\Gamma \approx 7/2$ polytrope and then at slightly lower densities ($\approx 10^{14} g/cm^{3}$) the EOS begins to soften with $\Gamma \approx 1/2$ and then at even lower densities, the EOS asymptotically approaches the adiabatic index of a relativistic Fermi gas, $\Gamma \approx 4/3$. In certain coordinates, these changes can happen very close to the neutron star surface and are difficult to resolve (\cite{deaton2013black}). Furthermore, initial data solves for BNS and NSBH binaries have traditionally used multi-domain spectral finite element methods such as the SpEC elliptic solver Spells (\cite{pfeiffer2003multidomain}) and LORENE (\cite{gourgoulhon2001quasiequilibrium}). Spectral methods have the nice feature of exponential convergence when the underlying problem is smooth. However, when the problem is non-smooth such as is the case when we consider tabulated EOSs or piecewise-analytic EOS fits (e.g. piecewise-polytropes), spectral methods show poor convergence due to Gibbs phenomenon. Unnatural schemes such as inserting subdomains by hand around the non-analytic parts are used to get converging solutions (\cite{deaton2013black}). 

Finally, for currently unknown reasons, Spells (\cite{pfeiffer2003multidomain}) doesn't converge for high compactness NS initial-data without multiple extra corrective iteration schemes and even then, it cannot converge to high accuracy data when the binary objects are very close in separation (\cite{henriksson2014initial}). This lack of convergence could be due to mathematical non-uniqueness in solutions of the XCTS formulation of the constraint equations (\cite{cordero2009improved}), but (\cite{henriksson2014initial}) hints at the possibility that the compactness can be pushed far beyond previous simulations by using refined iteration schemes. The goal of the SpECTRE elliptic solver will be to alleviate these issues by using more powerful, flexible techniques that scale well. In the next section we discuss a new powerful numerical technique.

\section{New numerical methods for solving the Einstein Initial Data Equations}

The most common methods used in Numerical Relativity to solve the evolution or initial data equations are finite difference, finite volume methods and spectral methods. We can understand all three methods by examining the following PDE:

\begin{equation}
  \label{eqn:residual}
 R(\bar u) = 0.
\end{equation}

Here $R$ is some partial differential operator, usually called the residual, that operates on the solution $u$ which could be composed of several fields we are solving for (e.g. the four fields in the initial data equations or the 10 components of the metric). Each of the different discretization methods used in numerical relativity demand in a different way, that the discrete version of Eqn.~\ref{eqn:residual} be zero. In the finite difference method we demand that $R(u)$ is zero at each of the grid points and then we Taylor expand the differential operators and represent the solution as a single number at each grid point. In finite volume methods we demand that the integral of the residual over an element (usually called a ``cell'') of a mesh is zero in each of the elements and we represent the solution as a single number per cell (the average of the solution over that cell). In the spectral element method we break up the domain into a set of elements with simple topologies and expand the solution over a set of basis functions and we demand that the residual is $L_2$-orthogonal to these basis functions. That is, on each subdomain we demand
%
\begin{equation}
  \label{eqn:l2orthog}
 \int R(\bar u)\psi_j  \mathrm{d}x = 0 \quad \forall \psi_j,
\end{equation}
%
which is just another way of setting the residual to be zero.

Recently a new method has been gathering ground. Discontinuous Galerkin (DG) methods (\cite{hesthaven2008nodal,kidder2016spectre}) combine the power of spectral methods in smooth regions with the shock-handling features of finite volume methods in discontinuous regions. To do this, DG methods represent the solution on each element as an expansion over a set of basis functions with the residual satisfying Eqn.~\ref{eqn:l2orthog}. To couple the solution between elements, DG methods use flux terms between neighboring elements which penalize deviation of the solution at the interface. These flux terms allow DG methods to borrow the discontinuity handling techniques of the finite volume method. With these features, DG can obtain exponential convergence even when the solution is not smooth over the entire grid (see Chapter 2 for more details). On top of this, DG methods have the following nice features

\begin{enumerate}
\item hp-adaptivity: In DG methods there are two types of refinement, you can increase the number of basis functions (p-refinement), or you can split an element of the mesh into smaller elements (h-refinement). In smooth regions, p-refinement is preferred and in non-smooth regions h-refinement is preferred. The combination of these two types of refinement (hp-refinement) can lead to rapid convergence of the solution.
\item Minimal communication: Since each element only needs the nearest-neighbour face data to compute the penalty flux terms, the amount of ghost data needed to be transferred between processors is minimal.
\item Easy Boundary handling: Boundary elements can be treated just like interior elements with the boundary conditions being applied through the penalty terms in the flux. This makes algorithms like Multigrid much easier to implement, since no special treatment is required for boundary elements.
\item Easy geometry handling: Unlike finite-difference methods which require special finite difference stencils for the boundaries of curved domains, DG methods can be applied as is to any type of domain, without any significant changes.
\end{enumerate}

In Chapter 2, we will be investigating the use of the Discontinuous Galerkin method on relevant problems in numerical relativity.

\section{Binary Neutron Star Simulations}

Neutron stars are amongst the most compact and extreme objects known in
the universe and are believed to be born from the result of
massive stars going supernova. Neutron stars have cores with densities exceeding far beyond that
of the nucleus and since similar conditions are unreachable on earth, neutron stars provide an exceptional testing ground for nuclear physics. In particular, the merger of two neutron stars provides us with an unique opportunity to study the high density region of the equation of state (EOS). Furthermore, as confirmed by GW170817, NS mergers are progenitors for short Gamma ray bursts (sGRBs) and the
heavy neutron-rich elements in the universe, whose synthesis in the fast moving neutron-star ejecta produces the optical and near-infrared EM counterparts that accompany the gravitational wave signal.

It is expected that with the increasing sensitivity of gravitational wave
interferometers multiple detections of merging BNSs will be made in the next years (\cite{ligo2018gwtc}). Therefore it will be crucial
to study the properties of these fascinating systems in order to extract information
from the data. While an analytical approach in general relativity is possible for the stage in which the two
bodies are distant and in the post-merger ringdown stage where the remnant loses its hair, numerical computation of the field equations is required
for the last few orbits of the inspiral and the merger. Thus it is imperative to have fully generalitivistic simulations of BNS mergers and NSBH mergers in order to maximize the science potential of the next era of detections.

The fully general-relativistic simulation of BNS mergers has now been possible for more than 19 years (\cite{shibata2000simulation}). However, despite continuous developments, current codes have not yet reached the accuracy required to model the gravitational wave signal and the electronmagnetic counterparts at the level required to extract as much information as possible from future detections (See (\cite{barkett2015gravitational})). Furthermore, most codes do not take into account all of the microphysics relevant to the evolution of the post-merger remnant, including a hot nuclear-theory based equation of state, a neutrino transport scheme accounting for both neutrino-matter and neutrino-neutrino interactions, and the evolution of the magnetic fields with enough resolution to resolve the growth of magneto-hydrodynamics instabilities (\cite{foucart2015low}). That being said, slowly but surely, different levels of micro-physics are being added to the codes. The first papers that studied fully general relativistic BNS simulations including the effects of neutrinos
were (\cite{neilsen2014magnetized, palenzuela2015effects}) with a simple leakage scheme and (\cite{sekiguchi2015dynamical}) with a more complex M1 neutrino transport scheme. Both the leakage scheme and M1 transport scheme are approximate methods to deal with neutrinos which are preferred because solving the 7-dimensional Boltzmann transport equation for the neutrino distribution during a BNS simulation is currently numerically intractable. For a review of leakage and the M1 transport scheme see (\cite{foucart2015post}). These BNS simulations with neutrino cooling and neutrino transport have focused mainly on equal mass systems with $M_{ns} = 1.35M_{\odot}$. Collectively these papers find that the ejected mass is only substantial  enough to explain the total mass of r-process heavy elements in our galaxy for r-process nucleo-synthesis in the case of a softer equation of state (e.g., more compact stars). The first paper on BNS mergers with neutrino interactions using the SpEC code looked at $1.2M_{\odot}$ equal mass systems and compared a simple leakage cooling neutrino scheme with the more complicated gray M1 neutrino transport scheme, finding that the more realistic transport scheme had a significant affect on the disk composition and the outflows, producing more neutron rich material that could possibly seed r-process element creation. (\cite{radice2016dynamical}) examined the effects of eccentricity and neutrino cooling on the matter outflows and remnant disk of a LS220 equal mass binary, finding that both had significant effects, with the absence of a neutrino scheme leading to matter outflows a factor of 2 off. Finally, only very recently have there been studies examining the effects on matter outflows due to mass asymmetry in the initial binaries, with both (\cite{lehner2016unequal}) and (\cite{sekiguchi2016dynamical}) finding that mass asymmetry can affect the neutron-richness and total ejecta for both soft and stiff equations of state.

In Chapter 3 of this thesis, we will be contributing to the above growing set of BNS studies, by looking at the effect of mass asymmetry and the EOS on matter and neutrino emissions using the SpEC code which now has a state-of-the-art neutrino transport scheme.

% Finite volume methods
% Finite volume (FV) methods were developed to solve PDEs in conservative form,
% ∂tu + ∇ · F®(u)  s, for a conserved quantity u with a flux vector F®(u) and source
% s. The equations of hydrodynamics — both Newtonian and relativistic — can be
% written in this form.
% In a FV method, the simulation domain is partitioned into cells. Cartesian
% grids are the norm, with each cell a small cubical volume in the domain. On
% this grid, the solution is discretized by encoding the cell-averaged value of the
% solution u at a grid point at the cell center. The flux F® is computed consistently
% at the interface between two neighboring cells, which results in a conservative
% method by construction. To obtain schemes with high accuracy, F® is computed
% using a broad stencil, i.e. using data from several cells; this is the problem of flux
% reconstruction. In the neighborhood of shocks, the FV method is prone to spurious
% oscillations and overshoots in the solution because of Gibbs’s phenomenon.
% So-called shock-capturing schemes ensure the solution remains physical in these
% regions.
% 7
% Today, FV methods are the standard technique for solving the equations of
% relativistic hydrodynamics in GR-hydro codes. This method is favored for its
% robustness and for the shock-capturing schemes that enable handling fluid shocks
% and stellar surfaces. The FV method nevertheless has inherent limitations when
% used as a high-order method: the large stencils required for the corresponding
% differencing and shock-capturing schemes make it difficult to adapt the grid to
% the problem geometry, and can also lead to challenges in efficiently parallelizing
% the algorithm.
% Many codes also use the FV method to solve the Einstein equations; although
% these PDEs cannot be written in conservative form, they take the similar hyperbolic
% form, and so much of the same formalism applies. The shock-capturing
% properties of the FV method are not needed for the smooth spacetime variables.
% Spectral methods
% Spectral methods also divide the computational domain into elements; these elements
% are typically large regions with simple topologies, such as cubes, spherical
% shells, etc. On each of these elements, a set of N polynomial basis functions is
% introduced. The solution u is expressed as an expansion over this basis. When
% the solution is a smooth function, the error in the expansion decreases exponentially
% as the order N is increased, giving rise to the exponential convergence of
% the spectral method. However, when there is a discontinuity in the solution u,
% the nice convergence properties of the method are lost. For this reason, spectral
% methods are not commonly used in fluid dynamics, where shocks can arise.
% Spectral methods are in use today in the Simulating eXtreme Spacetimes collab8
% oration’s Spectral Einstein Code (SpEC) to produce numerous binary black hole
% merger simulations. The high accuracy of the spectral method permits long (tens
% of orbits) and efficient inspiral simulations with excellent control of the errors
% in the waveforms. When simulating binaries that contain one or two neutron
% stars, SpEC uses the spectral method to evolve the spacetime and a FV method
% to evolve the matter . This dual-grid approach allows the spacetime to be treated
% accurately and efficiently, while still correctly handling the fluid with its shocks
% and surfaces. However, there is substantial computational expense associated
% with communicating data between the spectral and FV grids, and the difficulties
% facing the FV method still apply.
% Discontinuous Galerkin methods
% Discontinuous Galerkin (DG) methods are, in an informal sense, a hybrid between
% spectral methods and FV methods. From spectral methods, DG methods
% draw the representation of the solution, on each element, as an expansion over
% a set of basis functions. From FV methods, DG methods draw the concepts
% that enable robust handling of the hydrodynamics: the use of a unique flux
% between neighboring elements to ensure conservation, and the shock-capturing
% techniques to handle discontinuities in the solution. As a result, DG methods
% combine the properties of exponential convergence in regions where the solution
% is smooth with the ability to handle shocks. In addition, they present several
% other desirable qualities:
% 1. geometric flexibility: the grid can be deformed to conform to the symmetries
% of the problem, or to the shape of an external domain boundary;
% 2. hp-adaptivity: the grid resolution can be tailored to the problem by adapt9
% ing either the local order of approximation on the element (p-refinement),
% or the size of the (and the number of) elements (h-refinement); and,
% 3. local formulation: the method only requires exchanging data with nearestneighbor
% elements, simplifying communication patterns and enabling good
% scaling on large machines.
% The development of DG methods has undergone steady progress since the
% 1980s, with early emphasis on finding a stable formulation for non-linear conservation
% laws via the development of (low-order) shock-capturing schemes.
% More recently, in the early 2000s, work on more advanced WENO-based shockcapturing
% schemes [14, 15] promises to improve the accuracy of the method in
% problems with shocks. Paralleling these developments, the use of the DG method
% has expanded, with solutions to problems in electromagnetism, acoustics, plasma
% physics, gas dynamics, and atmospheric modeling.
% The application of the DG method to problems in relativistic astrophysics is
% recent and remains, thus far, exploratory in nature.
% The first use of a DG method for the evolution of spacetime geometry was
% by Zumbusch [16], who used a variational principle to obtain a space-time
% DG method for the linearized Einstein equations in harmonic gauge. For the
% commonly used Baumgarte-Shapiro-Shibata-Nakamura (BSSN) formulation of
% the Einstein equations, Field et al. [17] and later Brown et al. [18] developed
% DG methods in spherical symmetry. More recently, Miller and Schnetter [19]
% developed a DG method for the full BSSN equations in 3D, and showed success
% in evolving test problems.
% Efforts on the hydrodynamics side began with Radice and Rezzolla [20], who
% 10
% presented a formulation of DG for the evolution of fluids in curved spacetimes
% and evolved a neutron star in spherical symmetry. In their work, the spacetime
% is treated self-consistently by satisfying a radial constraint equation. In [21],
% Zhao and Tang implemented DG with a WENO shock-capturing scheme for
% special-relativistic hydrodynamics in 1D and 2D, and showed improved accuracy
% near shocks. Bugner et al. [22] were the first to apply DG to a 3D astrophysical
% fluid problem, evolving a neutron star in the Cowling approximation (in which
% the background metric remains fixed) and comparing different WENO schemes
% for handling of the star surface.
% Prior to the work reported here, the use of a DG method to solve simultaneously
% the coupled system of spacetime geometry and general-relativistic
% hydrodynamics has not been attempted. 
