\chapter{Introduction}
\label{chap:intro}


%https://arxiv.org/pdf/1803.07965.pdf
%https://arxiv.org/pdf/1710.05832.pdf
%https://journals.aps.org/prd/pdf/10.1103/PhysRevD.95.024029 
%Appendix D in Rezolla hydrodynamics good for Tabulated Equation of State and geometrized units
%Constraints on the neutron star equation of state from AT2017gfo using radiative transfer simulations -> good for intro paragraph
% Evaluating radiation transport errors in merger simulations using a Monte-Carlo algorithm
% https://arxiv.org/abs/1805.11581 GW170817: Measurements of neutron star radii and equation of state
% wyatts paper
% https://www.fis.unipr.it/gravity/HomePages/Thesis/PhDthesis_2009_SebastianoBernuzzi.pdf
% https://arxiv.org/pdf/1804.06308.pdf
% \subsection{Binary Neutron Star Mergers}
\cite{}
The first proposals for kilometer-scale gravitational wave interferometers were
formulated in the 1980s, and the scientific justification was based on two principal
potential sources:  the inspiral and merger of compact-object binaries, with neutron star and/or black hole components and supernova explosions. Since then, the LIGO \cite{ligo2018gwtc}, Virgo \cite{acernese2015advanced}, and GEO600 \cite{affeldt2014advanced} detectors have been funded, built, commissioned, and operated from 2005 to 2010 as a network in their initial stage of sensitivity – without detections at that stage. Alongside the early-2000 interferometer development, numerical simulations of the Einstein equations were beginning to gather ground. In 2000 the first binary neutron star coalescence was simulated \cite{shibata2000simulation}, five years later the first binary black hole coalescence was simulated \cite{pretorius2005a} and finally in 2006 the first binary neutron star - black hole simulations were performed \cite{shibata2011coalescence}. A variety of numerical relativity groups started forming at this time all around the globe, the Caltech-Cornell-CITA group \cite{sxswebsite},
the Kyoto group \cite{nagakura:2014hza}, the RIT group \cite{ritcatalog}, to name but a few. These groups started building banks of simulated waveforms
to aid in the parameter estimation studies that were expected to follow from the first detections. On September 9th, 2015, in a very unexpected event, LIGO detected the gravitational wave from two coalescing 30-$M_\odot$ black holes
in a ground-breaking 25 SNR event \cite{theligoscientific:2016wfe}. Since this day, there have been multiple binary black hole waveform detections \cite{ligo2018gwtc}. More recently however, there was the landmark discovery of GW170817 \cite{abbott2017gw170817}. This observation
coincided with the detection of a gamma ray burst, GRB 170817A \cite{2017apj...848l..13a}, verifying that the source binary contained matter, which was further corroborated by a series of observations that followed across the electromagnetic spectrum \cite{villar:2017wcc}. The measured masses of the bodies and the variety of electromagnetic observations are consistent with neutron stars (NSs).

With these detections, the gravitational era of astronomy has been born. Much more is expected in the future, with the upcoming earth-based detectors KAGRA and LIGO-India, there will be a worldwide detector network and soon there will third generation detectors such as the Einstein Telescope \cite{einsteintelescope}. With more and more detectors, both space-based and earth-based, we will be able to gather data from the full frequency spectrum of gravitational waves coming towards us.

Even though numerical relativity has matured greatly since the major developments of the early 2000's, there is still a lot more that needs to be done. The biggest problems today in numerical relativity are the simulation of binary neutron star mergers and the simulation of supernovae, both requiring a great amount of microphysics, multi-scale grids, large sets of complex non-linear PDES, large supercomputers and modern numerical techniques. With the coming of the exascale age of supercomputing there is an increasingly realistic chance of simulating these systems with all the known microphysics. This thesis aims to make progress in two distinct areas of  binary neutron star merger simulations. 1) Improve the computational techniques used solve the Einstein field equations so that more realistic microphysics may be introduced into the simulations as computing power reaches exascale and beyond. 2) Further probe the parameter space of binary neutron star simulations with one the most state of the art numerical relativity codes to help understand the emission properties of these LIGO sources for the next generation of detections. Chapter 1 of this thesis will address the former problem, Chapter 2 will address the latter. Finally, the remaining portion of this introductory chapter will briefly describe the physics and computational techniques needed to understand the main text.

\subsection{Solving the Einstein Field Equations on a Supercomputer}

In their usual form, space and time are treated on an equal footing in the Einstein equations. From the perspective of performing a numerical evolution however, we need to reformulate the problem as initial value problem where we have a set of initial gravitational and matter data at some time t, and a set of evolution equations which we can use to get the updated data at some other time. To do this, we make the ansatz that space-time can be treated as a time sequence of spatial hypersurfaces. With this ansatz, the space-time metric $g_{\mu\nu}$ can be decomposed into

\begin{equation}
\label{eq:4}
ds^{2} = g_{\mu\nu}dx^{\mu}dx^{\nu} = -\alpha^2dt^2 + \gamma_{ij}(dx^i+\beta^idt)(dx^j+\beta^jdt),
\end{equation}

where the spatial metric $\gamma_{ij}$ is a function of the spatial coordinates $x^{i}$ and $t$ and $\alpha$ is the lapse function that measures proper time between neighboring hypersurfaces along their timelike unit normals $n^{\mu}$ and $\beta^i$ is the shift vector that determines how coordinate labels move between each hypersurface. This is known as the 3+1 decomposition of the metric \cite{arnowitt2008republication}.

Now we need to decompose the Einstein field equations into a set of evolution and constraint equations involving the quantities $\beta^i$, $\alpha$ and $\gamma_{ij}$. To do this, we introduce the projection operator $\perp^{\alpha}_{\beta} = \delta^{\alpha}_{\beta} + n^{\alpha} n_{\beta}$, which can be easily proven to project the space-time components orthogonal to $n^{\mu}$ out of any space-time vector.

The three possible projections of the EFEs: $n^{\mu} n^{\nu} G_{\mu\nu} = 8\pi n^{\mu} n^{\nu} T_{\mu\nu}$, $n^{\mu}\perp^{\nu}_{\delta}G_{\mu\nu} = 8\pi n^{\mu} \perp^{\nu}_{\delta} T_{\mu\nu}$, $\perp^{\mu}_{\rho} \perp^{\nu}_{\delta} G_{\mu\nu} = 8\pi \perp^{\mu}_{\rho} \perp^{\nu}_{\delta} T_{\mu\nu}$ lead to the three York/ADM 3+1 equations respectively:

\begin{align}
0 &= R + K^{2} - K^{mn}K_{mn} -16\pi\rho, \\
0 &= D_iK - D_mK^m_i + 8\pi j_i, \\
\partial_t K_{ij} &= \beta^m\partial_mK_{ij} + K_{mj}\partial_i\beta^m + K_{im}\partial_j\beta^m - D_iD_j\alpha, \\ 
&+ \alpha(R_{ij}+KK_{ij}-2K_{im}K^m_{j})+4\pi\alpha[(S-\rho)\gamma_{ij}-2S_{ij}],
\end{align} 

where $K_{ij} = (\beta^m\partial_m\gamma_{ij} + \gamma_{mj}\partial_{i}\beta^{m} + \gamma_{im}\partial_j\beta^{m}-\partial_{t}\gamma_{ij})$, is called the extrinsic curvature and it measures
the rate at which the hypersurface deforms as it is carried forward along a normal \cite{baumgarte2010numerical}. We also relabelled the projections of stress tensor by $\rho = T_{\mu\nu} n^{\mu} n^{\nu}$, $j_{\alpha}= -\perp^{\nu}_{\alpha} T_{\mu\nu} n^{\nu}$, $S_{\alpha\beta} = \perp^{\mu}_{\alpha} \perp^{\nu}_{\beta} T_{\mu\nu}$ and $S=\gamma^{\mu\nu} S_{\mu\nu}$, while $R_{ij}$ and $R$ denote the Ricci tensor and scalar associated with $\gamma_{ij}$.

The 3+1 equations are a set of 10 equations, or 3 tensor equations. The 1st equation is known as the Hamiltonian constraint equation. The second tensor equation is called the momentum constraint equation and is composed of three equations. These four elliptic constraint equations play a similar role as the equations $\nabla \cdot \vec E = 4\pi\rho$ and $\nabla \cdot \vec B = 0$ which constrain the initial E and B fields in Maxwell's equations. The constraint equations must be solved prior to evolving the initial data and are usually called the initial data equations. The last set of six equations are the evolution equations. As they stand, the 3+1 equations still need to be manipulated a bit in order to solve them on a computer. For the set of evolution equations, the most used schemes are BSSN, Z4 and the Generalized Harmonic Decomposition (Generalized Harmonic decomp. technically doesn't use the ADM equations - see Chapter 2) which each manipulate the evolution equations into a slightly different well-posed hyperbolic system. For the initial data equations, the most used schemes are conformal tranverse traceless (CTT), conformal-thin sandwich (CTS) and the extended-conformal thin-sandwich (XCTS) frameworks \cite{pfeiffer-york:2005,alcubierre2012introduction,sopuerta2015gravitational}.

Alongside the field-equations, the matter equations must be solved. These come from the local conservation equations $\nabla_{\mu}T^{\mu\nu} = 0$. For Neutron-star matter, a perfect-fluid tensor and an irrotational velocity distribution are usually assumed and are coupled with some choice of equation of state (EOS). For black-holes, we set $\rho = j_{\alpha} = S_{\alpha\beta} = 0$ because there is only vacuum space. On top of the matter-equations, the equations of neutrino transport must be solved. To evolve the neutrinos in full generality

To extract the gravitational waveform, the Weyl scalar $\psi_{4}$, which represents the outgoing transverse radiation, is extracted at a large radius away from the simulated binary system (the ``wave-zone''). The energy, linear momentum, and angular momentum of the gravitational wave are computed by integrating the $\psi_{4}$ scalar in time \cite{kyutoku2015dynamical}.

For a more in-depth review, see \cite{sperhake2014numerical}, \cite{faber2012binary} and \cite{shibata2011coalescence} for BBH, NSNS and NSBH systems respectively.

\section{Current problems with Einstein Constraint equation solvers}

In simulations there are two current approaches to modeling neutron stars with temperature-dependent realistic EOSs in binaries. The first is to use nuclear-theory-based EOSs in tabulated form \cite{composewebsite} and to interpolate as required during the simulation. Since this can be computationally expensive, the more common approach is to use $n$ piece-wise polytropes with a polytropic thermal correction \cite{deaton2013black}, \cite{kyutoku2013black}, \cite{bauswein2014revealing}, \cite{kyutoku2015dynamical}. The cold piece-wise polytropic part is defined as follows,

\begin{equation}
\label{eq:7}
P_{cold} = K_i\rho^{\Gamma_i},\,\,\, \epsilon_{cold} = \epsilon_i + \frac{K_{i}\rho^{\Gamma_{i}-1}}{\Gamma_{i}-1},
\end{equation}

where $P$, $\rho$, $\epsilon$ and $K$ are the pressure, rest-mass density, specific internal energy and the polytropic constant, respectively, with i denoting the $i-th$ polytropic piece. It was found that four polytropic pieces ($n=4$) will approximate most EOSs of cold neutron matter to good accuracy \cite{read2008neutron}. To include thermal effects, one splits up the pressure $P = P_{cold} + P_{th}$ and internal energy $\epsilon = \epsilon_{cold}+\epsilon_{th}$ and assumes an ideal gas relationship $P_{th} = \rho \epsilon_{th}(\Gamma_{th}-1)$ where the ideal gas index $\Gamma_{th}$ is constant for all $\rho$ and $\epsilon$ and usually set to 2 \cite{bauswein2010testing},\cite{takami2014constraining}.


%\begin{equation}
%\label{eq:6}
%p(\rho) = K_i\rho^{\gamma_i}, \,\,\, d \frac{\epsilon}{\rho} = -pd \frac{1}{\rho}, \,\,\, \rho_i \leq \rho \leq \rho_{i+1}
%\end{equation}

%It was found that using a fixed SLY4 piecewise-polytropic fit for the low-density region of the NS, only three extra fixed polytropic regions were needed to fit all 31 tabulated EOSs (which include EOSs with exotic particles) in \cite{read2009constraints}. Most cold EOS models agree to within 1 percent on the low density section and thus this is usually fixed, with a SLY4 EOS is used.

 The majority of NS binary simulations to date, which includes the initial data solves, use smooth analytic EOSs such as the unrealistic $\Gamma = 2$ polytrope and stars with low compactness ($C = M_{NS}/R_{NS} \approx .1$) \cite{faber2012binary}. The main issue with solving the Einstein equations and matter equations with realistic EOSs in tabulated or piecewise-analytic form are that they are not smooth due to the multiple phase transitions and the non-analytic behavior or very steep slopes at the surface of the NS. For example, if we consider the LS EOS at low temperature and electron fraction \cite{lswebsite}, near the NS center the EOS can be approximated by a stiff $\Gamma \approx 7/2$ polytrope and then at slightly lower densities ($\approx 10^{14} g/cm^{3}$) the EOS begins to soften with $\Gamma \approx 1/2$ and then at even lower densities, the EOS asymptotically approaches the adiabatic index of a relativistic Fermi gas, $\Gamma \approx 4/3$. In certain coordinates, these changes can happen very close to the neutron star surface and are difficult to resolve \cite{deaton2013black}. Furthermore, initial data solves for BNS and NSBH binaries have traditionally used multi-domain spectral finite element methods such as LORENE \cite{gourgoulhon2001quasiequilibrium} and the SpEC elliptic solver \cite{pfeiffer2003multidomain}, \cite{ossokine2015improvements}. Spectral methods have the nice feature of exponential convergence when the underlying problem is smooth. However, when the problem is non-smooth such as is the case when we consider tabulated EOSs or piecewise-analytic EOS fits (e.g. piecewise-polytropes), spectral methods show poor convergence due to Gibbs phenomenon and unnatural schemes such as inserting subdomains by hand around the non-analytic parts are used to get converging solutions \cite{deaton2013black}. 

Finally, for currently unknown reasons, SpEC doesn't converge for high compactness NS initial-data without multiple extra corrective iteration schemes and even then, it cannot converge to high accuracy data when the binary objects are very close in separation \cite{henriksson2014initial}. This lack of convergence could be due to mathematical non-uniqueness in solutions of the XCTS formulation of the constraint equations \cite{cordero2009improved}, but \cite{henriksson2014initial} hints at the possibility that the compactness can be pushed far beyond previous simulations by using refined iteration schemes. The goal of the SpECTRE elliptic solver will be to alleviate these issues by using more powerful, flexible techniques that scale well.

\section{New numerical methods for solving the Einstein Constraint Equations}

The most common methods used in Numerical Relativity to solve the evolution or constraint equations in any of the frameworks (e.g. Generalized Harmonic or XCTS) are  finite difference, finite volume methods and spectral methods. We can understand all
three methods by examining the following general PDE:

\begin{equation}
 R(\bar u) = 0.
\end{equation}

Here $R$ is some partial differential operator, usually called the residual, on the solution $u$ which could be composed of multiple fields we are solving for (e.g. each of the 10 components of the metric and it's derivatives). Each of the different discretization methods demands in a different way, the equation to be zero and treats the discretization of derivatives and solution fields. In the Finite difference method we demand the $R(u)$ is zero at each of the grid points and then we Taylor expand the differential operators and represent the solution as a single number at each grid. In finite volume methods we demand that the integral of the residual over an element (usually called a ``cell'') of a mesh is zero in each of the elements and we represent the solution as a single number per cell (the average of the solution over that cell). In spectral element break up the domain into a set of elements with simple topologies and expand the solution methods we expand the solution in a power series of functions and we demand that the residual is $L_2$-orthogonal to these basis functions. That is on each subdomain we demand

\begin{equation}
 \int R(\bar u) \mathrm{d} \psi_j \bf{x} = 0. \forall \psi_j
\end{equation}

Between subdomains continuity is enforced. Recently a new method has been gathering ground. Discontinuous Galerkin (DG) methods are, in an informal sense, a hybrid between spectral methods and FV methods. From spectral methods, DG methods draw the representation of the solution, on each element, as an expansion over a set of basis functions. From FV methods, DG methods draw the concepts that enable robust handling of the hydrodynamics: the use of a unique flux between neighboring elements to ensure conservation, and the shock-capturing
techniques to handle discontinuities in the solution. As a result, DG methods combine the properties of exponential convergence in regions where
the solution is smooth with the ability to handle shocks or discontinuities. On top of this DG contains several other desirable qualities:

1. geometric flexibility: the grid can be deformed to conform to the symmetries of the problem, or to the shape of an external domain boundary;
2. hp-adaptivity: the grid resolution can be tailored to the problem by adapting either the local order of approximation on the element (p-refinement),
or the size of the (and the number of) elements (h-refinement); and,
3. local formulation: the method only requires exchanging data with nearestneighbor
elements, simplifying communication patterns and enabling good
scaling on large machines.


\subsection{Binary Neutron Star Simulations}

Neutron stars (NSs) are among the most compact objects in
the universe with central densities multiple times higher than
nuclear density. Similar conditions are unreachable on earth
which makes NSs an exceptional laboratory to test nuclear
physics predictions. In particular the merger of two NSs
allows the study of the high density region of the equation
of state (EOS) governing NS matter. In addition, NS mergers
also allow us to reveal the central engine for luminous short
Gamma ray bursts (sGRBs), to understand the origin of
heavy elements in the universe, which after their creation
produce the optical and near-infrared EM counterparts, called
kilonovae, and to test astrophysical predictions about binary
populations.

The first detection of gravitational waves (GW) combined
with an observation of a sGRB and a kilonova, interpreted as the merging of two binary neutron stars,  marks a breakthrough in the field of multi-messenger astronomy \cite{abbott2017gw170817}. It is
expected that with the increasing sensitivity of advanced GW
interferometers multiple GW detections of merging BNSs will
be made in the next years [3]. Therefore it will be crucial
to study the properties of these fascinating systems In order to extract information
from the data. While an analytical approach to the two-body dynamics
in general relativity is possible for the stage in which the
bodies are well-separated, a numerical solution of the field
equations, dealing with all their nonlinearities, is needed for
a faithful description of the last few orbits.


The fully general-relativistic simulation of BNS mergers has now been possible for more than 18 years \cite{shibata2000simulation}. However, despite continuous developments, current codes have not yet reached the accuracy required to model the gravitational wave signal at the level required to extract as much information as possible from future detections (See \cite{barkett2015gravitational}). Furthermore, most codes do not take into account all of the microphysics relevant to the evolution of the post-merger remnant, including a hot nuclear-theory based equation of state, a neutrino transport scheme accounting for both neutrino-matter and neutrino-neutrino interactions, and the evolution of the magnetic fields with enough resolution to resolve the growth of magneto-hydrodynamics instabilities \cite{foucart2015low}. That being said, slowly but surely, different levels of micro-physics are being added to the codes. The first papers that studied fully general relativistic BNS simulations including the effects of neutrinos
were \cite{neilsen2014magnetized, palenzuela2015effects} with a simple leakage scheme and \cite{sekiguchi2015dynamical} with a more complex M1 neutrino transport scheme. Both the leakage scheme and M1 transport scheme are approximate methods to deal with neutrinos which come about because solving the 7-dimensional Boltzmann transport equation for the neutrino distribution during a BNS simulation is currently numerically intractable. For a review of leakage and the M1 transport scheme used in SpEC, see \cite{foucart2015post}. These BNS simulations with neutrino cooling have focused solely on equal mass systems with $M_{ns} = 1.35M_{\odot}$. Collectively these papers find that the ejected mass is only substantial  enough to explain the total mass of r-process heavy elements in our galaxy for r-process nucleo-synthesis in the case of a softer equation of state (if $P ~ \rho^{\gamma}$, then a soft EOS implies $\gamma$ is small). The first paper on BNS mergers with neutrino interactions using the SpEC code looked at $1.2M_{\odot}$ equal mass systems and compared a simple leakage cooling neutrino scheme with the more complicated gray M1 neutrino transport scheme, finding that the more realistic transport scheme had a significant affect on the disk composition and the outflows, producing more neutron rich material that could possibly seed r-process element creation. \cite{radice2016dynamical} examined the effects of eccentricity and neutrino transport on the matter outflows and remnant disk of a LS220 equal mass binary, finding that both had significant effects, with the absence of a neutrino scheme leading to matter outflows a factor of 2 off. Finally, only very recently have there been studies examining the effects on matter outflows due to mass asymmetry in the initial binaries, with both \cite{lehner2016unequal} and \cite{sekiguchi2016dynamical} finding that mass asymmetry produces larger neutron-rich outflows for both soft and stiff equations of state.

% Finite volume methods
% Finite volume (FV) methods were developed to solve PDEs in conservative form,
% ∂tu + ∇ · F®(u)  s, for a conserved quantity u with a flux vector F®(u) and source
% s. The equations of hydrodynamics — both Newtonian and relativistic — can be
% written in this form.
% In a FV method, the simulation domain is partitioned into cells. Cartesian
% grids are the norm, with each cell a small cubical volume in the domain. On
% this grid, the solution is discretized by encoding the cell-averaged value of the
% solution u at a grid point at the cell center. The flux F® is computed consistently
% at the interface between two neighboring cells, which results in a conservative
% method by construction. To obtain schemes with high accuracy, F® is computed
% using a broad stencil, i.e. using data from several cells; this is the problem of flux
% reconstruction. In the neighborhood of shocks, the FV method is prone to spurious
% oscillations and overshoots in the solution because of Gibbs’s phenomenon.
% So-called shock-capturing schemes ensure the solution remains physical in these
% regions.
% 7
% Today, FV methods are the standard technique for solving the equations of
% relativistic hydrodynamics in GR-hydro codes. This method is favored for its
% robustness and for the shock-capturing schemes that enable handling fluid shocks
% and stellar surfaces. The FV method nevertheless has inherent limitations when
% used as a high-order method: the large stencils required for the corresponding
% differencing and shock-capturing schemes make it difficult to adapt the grid to
% the problem geometry, and can also lead to challenges in efficiently parallelizing
% the algorithm.
% Many codes also use the FV method to solve the Einstein equations; although
% these PDEs cannot be written in conservative form, they take the similar hyperbolic
% form, and so much of the same formalism applies. The shock-capturing
% properties of the FV method are not needed for the smooth spacetime variables.
% Spectral methods
% Spectral methods also divide the computational domain into elements; these elements
% are typically large regions with simple topologies, such as cubes, spherical
% shells, etc. On each of these elements, a set of N polynomial basis functions is
% introduced. The solution u is expressed as an expansion over this basis. When
% the solution is a smooth function, the error in the expansion decreases exponentially
% as the order N is increased, giving rise to the exponential convergence of
% the spectral method. However, when there is a discontinuity in the solution u,
% the nice convergence properties of the method are lost. For this reason, spectral
% methods are not commonly used in fluid dynamics, where shocks can arise.
% Spectral methods are in use today in the Simulating eXtreme Spacetimes collab8
% oration’s Spectral Einstein Code (SpEC) to produce numerous binary black hole
% merger simulations. The high accuracy of the spectral method permits long (tens
% of orbits) and efficient inspiral simulations with excellent control of the errors
% in the waveforms. When simulating binaries that contain one or two neutron
% stars, SpEC uses the spectral method to evolve the spacetime and a FV method
% to evolve the matter . This dual-grid approach allows the spacetime to be treated
% accurately and efficiently, while still correctly handling the fluid with its shocks
% and surfaces. However, there is substantial computational expense associated
% with communicating data between the spectral and FV grids, and the difficulties
% facing the FV method still apply.
% Discontinuous Galerkin methods
% Discontinuous Galerkin (DG) methods are, in an informal sense, a hybrid between
% spectral methods and FV methods. From spectral methods, DG methods
% draw the representation of the solution, on each element, as an expansion over
% a set of basis functions. From FV methods, DG methods draw the concepts
% that enable robust handling of the hydrodynamics: the use of a unique flux
% between neighboring elements to ensure conservation, and the shock-capturing
% techniques to handle discontinuities in the solution. As a result, DG methods
% combine the properties of exponential convergence in regions where the solution
% is smooth with the ability to handle shocks. In addition, they present several
% other desirable qualities:
% 1. geometric flexibility: the grid can be deformed to conform to the symmetries
% of the problem, or to the shape of an external domain boundary;
% 2. hp-adaptivity: the grid resolution can be tailored to the problem by adapt9
% ing either the local order of approximation on the element (p-refinement),
% or the size of the (and the number of) elements (h-refinement); and,
% 3. local formulation: the method only requires exchanging data with nearestneighbor
% elements, simplifying communication patterns and enabling good
% scaling on large machines.
% The development of DG methods has undergone steady progress since the
% 1980s, with early emphasis on finding a stable formulation for non-linear conservation
% laws via the development of (low-order) shock-capturing schemes.
% More recently, in the early 2000s, work on more advanced WENO-based shockcapturing
% schemes [14, 15] promises to improve the accuracy of the method in
% problems with shocks. Paralleling these developments, the use of the DG method
% has expanded, with solutions to problems in electromagnetism, acoustics, plasma
% physics, gas dynamics, and atmospheric modeling.
% The application of the DG method to problems in relativistic astrophysics is
% recent and remains, thus far, exploratory in nature.
% The first use of a DG method for the evolution of spacetime geometry was
% by Zumbusch [16], who used a variational principle to obtain a space-time
% DG method for the linearized Einstein equations in harmonic gauge. For the
% commonly used Baumgarte-Shapiro-Shibata-Nakamura (BSSN) formulation of
% the Einstein equations, Field et al. [17] and later Brown et al. [18] developed
% DG methods in spherical symmetry. More recently, Miller and Schnetter [19]
% developed a DG method for the full BSSN equations in 3D, and showed success
% in evolving test problems.
% Efforts on the hydrodynamics side began with Radice and Rezzolla [20], who
% 10
% presented a formulation of DG for the evolution of fluids in curved spacetimes
% and evolved a neutron star in spherical symmetry. In their work, the spacetime
% is treated self-consistently by satisfying a radial constraint equation. In [21],
% Zhao and Tang implemented DG with a WENO shock-capturing scheme for
% special-relativistic hydrodynamics in 1D and 2D, and showed improved accuracy
% near shocks. Bugner et al. [22] were the first to apply DG to a 3D astrophysical
% fluid problem, evolving a neutron star in the Cowling approximation (in which
% the background metric remains fixed) and comparing different WENO schemes
% for handling of the star surface.
% Prior to the work reported here, the use of a DG method to solve simultaneously
% the coupled system of spacetime geometry and general-relativistic
% hydrodynamics has not been attempted. 
